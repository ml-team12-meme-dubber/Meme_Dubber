#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Meme Dubber - A web application for generating teacher's audio from meme images 
Uses Google Gemini API for text extraction and gTTS/ChatTTS for audio generation,
Then use RVC for voice conversion.
"""

import os
import json
import sys
from io import BytesIO
from PIL import Image
import gradio as gr
from dotenv import load_dotenv
from inferrvc import RVC, load_torchaudio
import soundfile as sf
import torch
import fairseq.data.dictionary


# Load environment variables from .env file, which should contain GOOGLE_API_KEY
load_dotenv()

# Import Google Gemini SDK
from google import genai
from google.genai import types

# Get API key from environment
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')

if not GOOGLE_API_KEY:
    raise ValueError("GOOGLE_API_KEY not found in environment variables. Please set it in your .env file")


def extract_text_from_meme(image):
    """
    Extract text and language from meme image using Google Gemini API
    Args:
        image: PIL Image object
    Returns:
        tuple: (meme_text, lang_code)
    """
    try:
        # Initialize Gemini client
        client = genai.Client(api_key=GOOGLE_API_KEY)

        # Convert PIL Image to bytes
        img_byte_arr = BytesIO()
        image.save(img_byte_arr, format='PNG')
        img_byte_arr = img_byte_arr.getvalue()

        # System instruction for Gemini
        system_instruction = """
        You are an expert meme analyst.
        """

        # Call Gemini API
        response = client.models.generate_content(
            model='gemini-2.5-flash',
            contents=[
                types.Part.from_bytes(
                    data=img_byte_arr,
                    mime_type='image/png'
                ),
                """
                You are an expert meme analyst. Your task is to analyze the provided image.
                1.  First, determine if the image contains clear, readable text (e.g., captions, dialogue).
                2.  If it DOES contain text: Extract the text verbatim.
                3.  If it does NOT contain text (or the text is unreadable): Create a short, funny, meme-style dialogue that fits the scene, characters, and mood.
                4.  Identify the primary language of the extracted or generated text. Use standard language codes (e.g., 'en' for English, 'zh-tw' for Traditional Chinese, 'ja' for Japanese, 'es' for Spanish).
                5.  Return your response as a single JSON object with two keys: "language_code" and "text". Do not add any other explanatory text or formatting.

                Example for an English meme:
                {
                    "language_code": "en",
                    "text": "This is the text from the meme."
                }

                Example for a Japanese meme without text:
                {
                    "language_code": "ja",
                    "text": "é¢ç™½ã„ã‚»ãƒªãƒ•ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚"
                }
                """
            ],
            config=types.GenerateContentConfig(
                system_instruction=system_instruction,
                thinking_config=types.ThinkingConfig(
                    thinking_budget=1024 # æ€è€ƒçš„é ç®—ï¼Œå–®ä½ç‚ºtoken
                ),
                temperature=0.7, # éš¨æ©Ÿæ€§
                response_mime_type="application/json" # æŒ‡å®šå›æ‡‰æ ¼å¼ç‚ºJSON
            )
        )

        # Parse response
        print(f"Gemini response: {response.text}")  # Add debug info
        result = json.loads(response.text)
        meme_text = result.get('text', '')
        lang_code = result.get('language_code', 'en')
        
        # ç¢ºä¿è¿”å›æœ‰æ•ˆçš„æ–‡æœ¬
        if not meme_text or meme_text.strip() == "":
            print("Warning: Empty text extracted from meme")
            return None, "en"

        return meme_text, lang_code

    except Exception as e:
        print(f"Error extracting text from meme: {e}")
        import traceback
        traceback.print_exc()
        return None, "en"  


def generate_audio_gtts(text, lang_code):
    """
    Generate audio using Google Text-to-Speech (gTTS)
    Args:
        text: Text to convert to speech
        lang_code: Language code (ISO 639-1)
    Returns:
        str: Path to generated audio file
    """
    try:
        from gtts import gTTS

        # Generate speech
        tts = gTTS(text=text, lang=lang_code)

        # Create full audio file path
        audio_filename = "meme_audio_gtts.mp3"
        audio_file = os.path.join(os.getcwd(), audio_filename)
        
        # Debug information
        print(f"Current working directory: {os.getcwd()}")
        print(f"Audio file path: {audio_file}")
        print(f"Is directory: {os.path.isdir(audio_file)}")
        
        # Save audio file
        tts.save(audio_file)
        
        # Verify file was created
        if os.path.exists(audio_file):
            print(f"âœ“ Audio file created successfully: {audio_file}")
        else:
            print(f"âœ— Failed to create audio file")
            return None

        return audio_file

    except Exception as e:
        print(f"Error generating audio with gTTS: {e}")
        import traceback
        traceback.print_exc()
        return None


def generate_audio_chattts(text, lang_code):
    """
    Generate audio using ChatTTS
    Args:
        text: Text to convert to speech
        lang_code: Language code (ISO 639-1)
    Returns:
        str: Path to generated audio file
    """
    try:
        import ChatTTS
        import soundfile as sf

        # Load ChatTTS model
        chat = ChatTTS.Chat()
        chat.load(compile=False)

        # æ¡æ¨£éš¨æ©Ÿèªªè©±çš„äººè² 
        # rand_spk = chat.sample_random_speaker()  
        # print("Randomly sampled speaker embedding:!!!!!!!")
        # print(rand_spk)
        fix_spk = "è˜æ·°æ•¥æ¬€æ¡ƒæ¹¤ç·•ç­æ†­ç†•å½è¤¥èª¼æ¤ç°¬å¶ç”¤æ«¼è«°ç ƒæ«ç‹¶ç› èª¸å‡„å‹§å˜ªçˆŸå‘­å›å†²ç —ä¾¤è†è­è«¶å‘”æ¡Šåè™†å‘´è¦ æ¬»æ¯Ÿè‰å½³æ‰Œç•©å„ç‚‘è–§æ˜¬æ®¥ç¢«æš¢æ‰±ç˜²ä¹ºçˆµåºè©æ›ˆå¹™å„ç©ç–­åº‚æ¢»å³‹å‹©è€­ç­¶æ £å—¤ç¤¥ç¡˜ç®¢è³¹æ¡‰æŸæœ£è¦‹æ‹•åª—çµ‡å…™å ¾åœå§®è—å“»å®å¨è‹ˆè˜°ç¶§å•¾è¶å‘è±ªç ¼è„¯ç«ˆåˆ¸ç†¢è¨æ²½çƒ¯è•‚èä¿Œå°è™æ¸·å†ŸåµŒè¡§æ¦†æº—æ‡†æŠƒå©¼åƒ‡åˆ›ç½è†‡æ¡³èƒè˜›æœ‹åœ„çˆå…¹æ¬¨æ„°æ“çœ—ç¿¤å—è“´æ¥¼æ–˜ç˜¥ç¸·èŸè·¬è«®ç–æ¢¹ç”€ç‚å´‚çš•åº¬ç Œå‘“èƒ¦è²¬è‡½è­Ÿæ·›å¡›å°©ç¡‡æ“ŒèŒæ…’ç›å¡å–è¨ä¾…æª…åˆæ´ä¿´ç¹¼ç´¯å›®æ¡¥åª³å™µå¿€å¯·åˆ„å‹“å´¶è¡ç¦¢æ˜“ä»¿å­†è­…èŠ›ç»‡å„«æ¯»ç…ºè•¤æ­£æ¢å°­æ¶†å­¶è¨‘å è›¯æ€æ¨ç±»æ‹¿ç°Ÿå¶‚è®µä½²ç„©å™¦èæ»¡çº¯æ‚¶ç±å†‰æŒªå• è±³è¥å„±ç±‘å¶¤ç®æ¸˜æŸ„å™ºç¡æ©‡è¾æ…—å¨£æ¾¡æ¨²ç‡¾è‡¶ç™¤ç‡²ç•½æ´‡è‰æ¶²ç –å˜æ¤‘å¡…è«¼è¥¶æª£å·›ä¿¹æ½­å”„ç€æ²–ç §çƒ·çœå¨³å™µä¾ƒå¢Šè‚‚å­¦çŒ’ç²‡æ„‡äº’æ¾©ç´“ç¬–è¯¯å±•æˆ–æ–›æ¸˜è³™è‰›æ¹›ç…ç¾¶å‹²æ°šç‡™ç©½æ™Œä»è·è¶šå«€ç“¶ç±–å«ŠåŒµç˜åå˜£è‰ˆè°å”è© çºç²åµ©æ¿«æ‰«çªµæ«·çœ¡ç²æ¶å›¨ç¤è®™ç„æš¡å»è—–ç®—æ¦å‹æ²ºè†¼ç› ä¾¤æ…­è² è€§æ››æ·¡å‚¶å¬¸è‘Ÿè´¨æ‡¼è¶‰æ¤»æ•¯èªç§¨åˆ å‚æ‰å’¦è¦¼ç©¡ç´µæµ£çƒ¤å«±æ´ç¯³åº„å¦¡æ¶åº±å·—ç°¡ç‰»ç¡¢æ”¡æ’©åçŒ½èŠ£æ•¦æ±˜è¢ƒå«½è§§æ°©èªæ³§æ¡¿ç¦¤çƒ’åœµç¿›æ±„è•åµçŸå›ä¼®èŸ˜èå«®æ‹±ç£·æ¶€çå»Šè‡¬æ‚å¹£ç‚æ”æ”£æŠˆè„£è–¬è‡æ²¨åšåŠ™ä¼Šæ˜æ”•ç·¸è•¾å½‡è˜™çŠå‰šç°Ÿç‚å ˜è“µå¾‘çŠç‰è™Œæ†©è›¥æ¬¯è ç‰¦å³§æ¾å£ç»¥ç³˜æ£ç“»å‹Ÿåç„³è„æ¿å¬¡å±™æ˜ˆç¿æˆç™‰ç´šå¸¦è  å¦œå¯å•ç’¥æ¿’å¬å´¯ç«Œç¤Œæº§åŸæ¾šèŸå§›æ±ªå¤æ­æ¨“æ¾‹è‹®ç•±æ»Šè°ˆçœ¸ç¯¬æ©°ä¾æ­‰çºŸè®¯å—œç¤ƒæ¾¿ä¿“æ¬±åšŸå†è¥“åˆä¼ˆä¸›è°’æ‡•èª·æ±å½‹å±ç˜§å‰ç¥è¶¶ç¯¾åš¾è­å‹»è«è£¡åƒç²æŸ›è·ªæ‚´å¯‰ç‰¨ç»³å¶µè§·åŠ¾æ¯„æ¹Šæ»šæ¤³å¶å¼£æ„¸çš»ç”‰ç´ æ¶­æ‘¾æƒçŒ»æ‡¸å«°å«®æ‚¨è„¯æ­å’ç®å ¸ç‘®è·æ›„åˆ—æ‡”æˆ¯è«ºç§¶ç¾¿æ•˜è¦„ç•¢ç¼ºæ¶µè²¸åŒä¸±ç£ªæ±è¦³è˜…è‘›çŠ“ç°æˆ„èš¶è¡˜è’è‹šç®¼æ¯å²‘æ¯ºæ“ç·å˜»è·³åŠ¤ç¹¯èµ³å¯´è–°ç¼¥å¼å„ˆç™•æ•³å„…åŒ–ä½›å¿è’©æ«—ç–¨çšç¦¡è­³èš¾æ–‹è¶’æ³ç˜’å–ç¤ªå®¶ç‚ªæ‰‘åµ´å“æ©¶æ›±è²…æ¿æ”„è—æŸè¼æ¼£æ®Ÿè²¼æ˜¯å‹œçš’è¶¤å«‡ç“’æ·’æ˜¯ç‰–ç³»å€¶å¾™ç²‚çœ˜æ«™ç‡£æ´¡åå¤…å¶ä¹¡æƒ‡ç¥›æ¨¡æ¸‘æ¾ˆæ©ç»†æº‰æŒ–ç¯‰ä¾çŠæ“¬ç¯—è¡¬æå¸¬æ¦ æ¤¦è©‚ç¨‚æ®”ç„å“ èƒ‰è…³æˆºåº…ç…ç•¹åç¦Ÿç¡¥çœæ¢£èš‚å„ç«±å±‡å©…ççµ±ç—«å„·å¶“æ²åº¹ç»¢å¡¦ç¶«è¥è§•å–Ÿè·Ÿè£¤èœç¦“å‘æ—ºèœ²è˜šæª¶ç†–å––æ°€åªç¤•è’¡æ¡³çšåŠ†å¢„ç ²ç«½ç´˜å·šè«¾è›åçŠ¢å’ä¼®è±±èŸåœªå¬›ç³–çŠæº²å·åŒ°ä»¬æŒ³æ²½æ¸æ¸ç¹™ç‹‚è’ä»ªå‹»æ¡‰è¶˜æ¹”æ¥¬çŒèœ‰ç†ƒè—¯çº»æ¸®è£Ÿæ¤‰è¢‰å·æ‹¥å‘“å†¼ççœæ®‹ä¼å¿šè©›æ­—è‰—è‚¡èŸ­ä¹®å¯ƒç¿¤å¡ä»ˆå¥®è­¡å‹çª¤æ§šæ”­æ¶”è´©å‡Ÿæ›½ç‹¢ç•³è› çŸ³ç¨¸åµ‚è«‰æ²¥ç›¼å¶•è•è‰¦å™°ç­æ›´æ°ºæ¸æ’•ç³ªå”…å·æ›°æ»ƒè¥ç ‹è‹„èƒƒèª”å¹ç½±ç­ºæ ƒç—¼æ——ç® åŸ½ç¸˜ä¾˜è…å¬ç½¦ç”’è¶‰çœ²å´–çˆ¼æ³¦èå®›æ¡±æ§€è½èƒ«è£¢è¢‰ä¸–å¦”ç©ç¨¢è·—çœ¥ç”¯ä¾–æ’©åŠšç¡¶ç ™å¨å® ä¸€"

        params_infer_code = ChatTTS.Chat.InferCodeParams(
            spk_emb=fix_spk,  # ä½¿ç”¨å›ºå®šçš„èªªè©±äººåµŒå…¥
            # spk_emb=rand_spk,  # ä½¿ç”¨æ¡æ¨£çš„èªªè©±äººåµŒå…¥
            temperature=.00001,
            top_P=0.7,
            top_K=20,
        )

        # Generate speech
        wavs = chat.infer([text], params_infer_code=params_infer_code)

        # Chatttsæœ‰æ™‚å€™éŸ³é‡å¤ªå°è²ï¼ 
        # è‡ªå‹•å°‡æœ€å¤§éŸ³é‡ç¸®æ”¾åˆ° 1.0
        wav = wavs[0]
        import numpy as np
        current_peak = np.max(np.abs(wav)) if wav.size else 0.0
        if current_peak > 0:
            wav = wav / current_peak  # ç¸®æ”¾æœ€å¤§éŸ³é‡åˆ° 1.0

        # Create full audio file path
        audio_filename = "meme_audio_chattts.wav"
        audio_file = os.path.join(os.getcwd(), audio_filename)
        
        # Debug information
        print(f"Current working directory: {os.getcwd()}")
        print(f"Audio file path: {audio_file}")
        
        # Save audio file using soundfile
        sf.write(audio_file, wavs[0], 24000)
        
        # Verify file was created
        if os.path.exists(audio_file):
            print(f"âœ“ Audio file created successfully: {audio_file}")
        else:
            print(f"âœ— Failed to create audio file")
            return None

        return audio_file

    except Exception as e:
        print(f"Error generating audio with ChatTTS: {e}")
        import traceback
        traceback.print_exc()
        return None


def process_meme(image, tts_engine):
    """
    Main function to process meme image and generate audio

    Args:
        image: PIL Image object from Gradio
        tts_engine: TTS engine to use ("gTTS" or "ChatTTS")

    Returns:
        tuple: (extracted_text, audio_file_path)
    """
    if image is None:
        return "**Error:** Please upload an image first.", None

    # Extract text from meme
    meme_text, lang_code = extract_text_from_meme(image)

    # æª¢æŸ¥æ–‡æœ¬æå–æ˜¯å¦æˆåŠŸ
    if not meme_text or meme_text is None:
        return "**Error:** No text found in the image. Please try another image.", None

    # Generate audio based on selected TTS engine
    try:
        if tts_engine == "gTTS":
            audio_file = generate_audio_gtts(meme_text, lang_code)
        else:  # ChatTTS
            audio_file = generate_audio_chattts(meme_text, lang_code)
        
        # æª¢æŸ¥éŸ³æª”æ˜¯å¦ç”ŸæˆæˆåŠŸ
        if audio_file is None:
            result_text = f"**Extracted Text:** {meme_text}\n\n**Language:** {lang_code}\n\n**Error:** Failed to generate audio"
            return result_text, None
        
        result_text = f"**Extracted Text:** {meme_text}\n\n**Language:** {lang_code}"
        return result_text, audio_file
        
    except Exception as e:
        error_text = f"**Error generating audio:** {str(e)}\n\n**Extracted Text:** {meme_text}\n\n**Language:** {lang_code}"
        print(f"Error in process_meme: {e}")
        import traceback
        traceback.print_exc()
        return error_text, None

def rvc_convert(input_audio_path, f0_key):
    """
    Convert audio to teacher voice using RVC
    Args:
        input_audio_path: Path to meme_audio_xxx.wav
        f0_key: int, pitch shift (-24 ~ 24)
    Returns:
        str: Path to converted audio (meme_audio_teacher.wav)
    """

    print("=== RVC Inference Start ===")

    base_dir = os.path.dirname(os.path.abspath(__file__))
    model_path = os.path.join(base_dir, "model", "Teacher_infer.pth")
    index_path = os.path.join(base_dir, "index", "Teacher_infer.index")

    output_path = os.path.join(os.getcwd(), "meme_audio_teacher.wav")

    # fairseq fix
    try:
        torch.serialization.add_safe_globals([fairseq.data.dictionary.Dictionary])
    except:
        pass

    # Decide device
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print("Device:", device)

    # Load RVC model
    model = RVC(model_path, index=index_path)
    print("Loaded RVC:", model.name)

    # Load input audio
    audio, sr = load_torchaudio(input_audio_path)

    # Run conversion
    converted = model(
        audio,
        f0_up_key=int(f0_key),     # â˜… åŠ å…¥éŸ³é«˜æ§åˆ¶
        output_device="cpu",
        output_volume=RVC.MATCH_ORIGINAL,
        index_rate=0.5
    )

    if isinstance(converted, torch.Tensor):
        converted = converted.detach().cpu().numpy()

    # Save output
    sf.write(output_path, converted, 44100)
    print("Saved:", output_path)

    return output_path


def create_gradio_interface():
    """
    Create and configure Gradio web interface

    Returns:
        gr.Blocks: Gradio interface
    """
    with gr.Blocks(title="Meme Dubber", theme=gr.themes.Soft()) as demo:
        gr.Markdown(
            """
            # ğŸ˜œ Meme Dubber

            Upload a meme image and generate audio dubbing using AI!

            **How it works:**
            1. Upload a meme image
            2. Select your preferred TTS engine (gTTS or ChatTTS)
            3. Click "Generate Audio Dub"
            4. Listen to the AI-generated voiceover!
            """
        )

        with gr.Row():
            with gr.Column():
                # Input components
                image_input = gr.Image(
                    label="Upload Meme Image",
                    type="pil",
                    sources=["upload", "clipboard"]
                )

                tts_selector = gr.Radio(
                    choices=["gTTS", "ChatTTS"],
                    value="gTTS",
                    label="TTS Engine",
                    info="gTTS: Fast and simple | ChatTTS: More natural sounding"
                )

                pitch_slider = gr.Slider(
                                    minimum=-24,
                                    maximum=24,
                                    value=-5,
                                    step=1,
                                    label="Pitch Shift (F0 Key)",
                                    info="Adjust voice pitch (-24 ~ 24)"
                                )

                generate_btn = gr.Button("ğŸ¬ Generate Audio Dub", variant="primary", size="lg")
                rvc_btn = gr.Button("ğŸ¤ Convert to Teacher Voice", variant="secondary")

            with gr.Column():
                # Output components
                text_output = gr.Markdown(label="Extracted Text")
                audio_output = gr.Audio(label="Generated Audio", type="filepath")
                rvc_audio_output = gr.Audio(label="Teacher Voice Audio", type="filepath")

        # Set up event handler
        generate_btn.click(
            fn=process_meme,
            inputs=[image_input, tts_selector],
            outputs=[text_output, audio_output]
        )

        def run_rvc(audio_file, f0_key):
            if audio_file is None:
                return None
            return rvc_convert(audio_file, f0_key)
        
        rvc_btn.click(
            fn=run_rvc,
            inputs=[audio_output, pitch_slider],
            outputs=[rvc_audio_output]
        )

        gr.Markdown(
            """
            ---
            ### Notes:
            - **gTTS**: Google Text-to-Speech - Fast, cloud-based, supports many languages
            - **ChatTTS**: More natural sounding but requires more processing power
            - Supported languages: English, Chinese, Japanese, Spanish, and more!
            """
        )

    return demo


def main():
    """
    Main function to launch the application
    """
    print("Starting Meme Dubber...")
    print(f"API Key configured: {'âœ“' if GOOGLE_API_KEY else 'âœ—'}")

    # Create and launch Gradio interface
    demo = create_gradio_interface()
    demo.launch(
        server_name="127.0.0.1",
        share=False,
        show_error=True
    )


if __name__ == "__main__":
    main()
